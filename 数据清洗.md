# TF-IDF
## 核心步骤
### 1、数据预处理
* 分词与去噪
  使用jieba对文本数据进行分词，去除停用词
### 2、构建文档-词项矩阵
* 统计词频（TF）
  计算每个词在单个文档中的出现频率：
$$TF(t,d)=\frac{n}{N}$$
  其中，(t,d)中的t表示的是文档中的词汇，d表示的是文档中的词汇集合，n表示某词在文档中出现了n次，而文档中总共包含N个词
```python
  def tf(word_dict,doc_words):
    tf_dict = {}
    words_len = len(doc_words)
    for word,count in word_dict.items():
        tf_dict[word] = count/words_len
    return tf_dict
```
* 计算逆文档频率（IDF）
  衡量词在整个语料库中的稀有程度,IDF越高，说明词就越稀有。逆文档频率是说一个词的文档集合中出现的次数越少，它就具有表征性，因为在文中有很多“的”、“了”这种词，这些词重要性不大，反而出现少的词重要性大一些，IDF公式如下：
  $$IDF(t) = \log_2{\frac{D}{df_t+1}}$$
  其中，D是文档总数，$df_t$是包含词t的文档数量。通过取对数，可以避免数值过大的问题，同时保证了IDF的单调递减特性
```python
  def compute_idf(doc_list):
    sum_list = list(set([word_i for doc_i in doc_list for word_i in doc_i]))
    idf_dict = {word_i: 0 for word_i in sum_list}
    for word_j in sum_list:
        for doc_j in doc_list:
            if word_j in doc_j:
                idf_dict[word_j] += 1
    return {k: math.log(len(doc_list) / (v + 1)) for k, v in idf_dict.items()}
```
  
### 3、计算TF-IDF值
* 将TF与IDF想乘，得到每个词的权重
  $$TF-IDF = TF * IDF$$
```python
  def tf_idf(tf_dict,idf_dict):
    tfidf = {}
    for word,tf_value in tf_dict.items():
        tfidf[word] = tf_value * idf_dict[word]
    return tfidf
```
### 4、筛选与清洗
* 设定阈值：保留TF-IDF值高于阈值的词，过滤低权重词以减少噪声
#### 根据均值与标准差法的阈值确定方法
* 方法背景：
  在TF-IDF特征空间中，文档的最高权重反映了其核心关键词的重要性。可通过均值和标准差量化数据集中权重的离散程度，并设定阈值过滤低权重文档。
* 数学定义：
  设文档集合$D={d_1,d_2,...,d_n}$,其中每个文档$d_i$的最高权重TF-IDF权重为$\omega_i$。定义以下统计量：
  $$均值\mu=\frac{1}{n}\sum_{i=1}^{n} \omega_i$$
  $$标准差\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(\omega_i-\mu)^2}$$
  $$阈值\theta=\mu-k\cdot\sigma$$
  其中，k为调整系数，用于控制过滤的严格程度。系数k越大，保留的文档越多。
  ```python
  means = np.mean(max_weight_list)
  std_weight = np.std(max_weight_list)
  threshold = means-0.5*std_weight
  ```
* 重构数据集：仅保留筛选后的词项，形成清洗后的文本集合
